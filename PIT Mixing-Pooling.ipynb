{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\r\n",
    "import numpy as np\r\n",
    "import paddle.optimizer as opt\r\n",
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "\r\n",
    "from paddle.nn.initializer import TruncatedNormal, Constant, Assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trunc_normal_ = TruncatedNormal(std=.02)\r\n",
    "zeros_ = Constant(value=0.)\r\n",
    "ones_ = Constant(value=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Identity(nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(Identity, self).__init__()\r\n",
    "\r\n",
    "    def forward(self, input):\r\n",
    "        return input\r\n",
    "\r\n",
    "\r\n",
    "class Mlp(nn.Layer):\r\n",
    "    def __init__(self,\r\n",
    "                 in_features,\r\n",
    "                 hidden_features=None,\r\n",
    "                 out_features=None,\r\n",
    "                 act_layer=nn.GELU,\r\n",
    "                 drop=0.):\r\n",
    "        super().__init__()\r\n",
    "        out_features = out_features or in_features\r\n",
    "        hidden_features = hidden_features or in_features\r\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\r\n",
    "        self.act = act_layer()\r\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\r\n",
    "        self.drop = nn.Dropout(drop)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.fc1(x)\r\n",
    "        x = self.act(x)\r\n",
    "        x = self.drop(x)\r\n",
    "        x = self.fc2(x)\r\n",
    "        x = self.drop(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drop_path(x, drop_prob=0., training=False):\r\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\r\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\r\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ...\r\n",
    "    \"\"\"\r\n",
    "    if drop_prob == 0. or not training:\r\n",
    "        return x\r\n",
    "    keep_prob = paddle.to_tensor(1 - drop_prob)\r\n",
    "    shape = (paddle.shape(x)[0], ) + (1, ) * (x.ndim - 1)\r\n",
    "    random_tensor = keep_prob + paddle.rand(shape, dtype=x.dtype)\r\n",
    "    random_tensor = paddle.floor(random_tensor)  # binarize\r\n",
    "    output = x.divide(keep_prob) * random_tensor\r\n",
    "    return output\r\n",
    "\r\n",
    "\r\n",
    "class DropPath(nn.Layer):\r\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, drop_prob=None):\r\n",
    "        super(DropPath, self).__init__()\r\n",
    "        self.drop_prob = drop_prob\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        return drop_path(x, self.drop_prob, self.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Layer):\r\n",
    "    def __init__(self,\r\n",
    "                 dim,\r\n",
    "                 num_heads=8,\r\n",
    "                 qkv_bias=False,\r\n",
    "                 qk_scale=None,\r\n",
    "                 attn_drop=0.,\r\n",
    "                 proj_drop=0.):\r\n",
    "        super().__init__()\r\n",
    "        self.num_heads = num_heads\r\n",
    "        head_dim = dim // num_heads\r\n",
    "        self.scale = qk_scale or head_dim**-0.5\r\n",
    "\r\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias_attr=qkv_bias)\r\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\r\n",
    "        self.proj = nn.Linear(dim, dim)\r\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        # B= paddle.shape(x)[0]\r\n",
    "        N, C = x.shape[1:]\r\n",
    "        qkv = self.qkv(x).reshape((-1, N, 3, self.num_heads, C //\r\n",
    "                                   self.num_heads)).transpose((2, 0, 3, 1, 4))\r\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\r\n",
    "\r\n",
    "        attn = (q.matmul(k.transpose((0, 1, 3, 2)))) * self.scale\r\n",
    "        attn = nn.functional.softmax(attn, axis=-1)\r\n",
    "        attn = self.attn_drop(attn)\r\n",
    "\r\n",
    "        x = (attn.matmul(v)).transpose((0, 2, 1, 3)).reshape((-1, N, C))\r\n",
    "        x = self.proj(x)\r\n",
    "        x = self.proj_drop(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Block(nn.Layer):\r\n",
    "    def __init__(self,\r\n",
    "                 dim,\r\n",
    "                 num_heads,\r\n",
    "                 mlp_ratio=4.,\r\n",
    "                 qkv_bias=False,\r\n",
    "                 qk_scale=None,\r\n",
    "                 drop=0.,\r\n",
    "                 attn_drop=0.,\r\n",
    "                 drop_path=0.,\r\n",
    "                 act_layer=nn.GELU,\r\n",
    "                 norm_layer='nn.LayerNorm',\r\n",
    "                 epsilon=1e-5):\r\n",
    "        super().__init__()\r\n",
    "        self.norm1 = eval(norm_layer)(dim, epsilon=epsilon)\r\n",
    "        self.attn = Attention(\r\n",
    "            dim,\r\n",
    "            num_heads=num_heads,\r\n",
    "            qkv_bias=qkv_bias,\r\n",
    "            qk_scale=qk_scale,\r\n",
    "            attn_drop=attn_drop,\r\n",
    "            proj_drop=drop)\r\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\r\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else Identity()\r\n",
    "        self.norm2 = eval(norm_layer)(dim, epsilon=epsilon)\r\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\r\n",
    "        self.mlp = Mlp(in_features=dim,\r\n",
    "                       hidden_features=mlp_hidden_dim,\r\n",
    "                       act_layer=act_layer,\r\n",
    "                       drop=drop)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\r\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Layer):\r\n",
    "    def __init__(self, base_dim, depth, heads, mlp_ratio,\r\n",
    "                 drop_rate=.0, attn_drop_rate=.0, drop_path_prob=None):\r\n",
    "        super(Transformer, self).__init__()\r\n",
    "        self.layers = nn.LayerList([])\r\n",
    "        embed_dim = base_dim * heads\r\n",
    "\r\n",
    "        if drop_path_prob is None:\r\n",
    "            drop_path_prob = [0.0 for _ in range(depth)]\r\n",
    "\r\n",
    "        self.blocks = nn.LayerList([\r\n",
    "            Block(\r\n",
    "                dim=embed_dim,\r\n",
    "                num_heads=heads,\r\n",
    "                mlp_ratio=mlp_ratio,\r\n",
    "                qkv_bias=True,\r\n",
    "                drop=drop_rate,\r\n",
    "                attn_drop=attn_drop_rate,\r\n",
    "                drop_path=drop_path_prob[i],\r\n",
    "                norm_layer='nn.LayerNorm',\r\n",
    "                epsilon=1e-6,\r\n",
    "            )\r\n",
    "            for i in range(depth)])\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        n, c, h, w = x.shape\r\n",
    "        x = x.transpose((0, 2, 3, 1))\r\n",
    "        x = paddle.flatten(x, start_axis=1, stop_axis=2)\r\n",
    "\r\n",
    "        for blk in self.blocks:\r\n",
    "            x = blk(x)\r\n",
    "\r\n",
    "        x = x.transpose((0, 2, 1))\r\n",
    "        x = x.reshape((n, c, h, w))\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class conv_head_pooling(nn.Layer):\r\n",
    "    def __init__(self, in_feature, out_feature, stride,\r\n",
    "                 padding_mode='zeros'):\r\n",
    "        super(conv_head_pooling, self).__init__()\r\n",
    "\r\n",
    "        self.conv = nn.Conv2D(in_feature, out_feature, kernel_size=stride + 1,\r\n",
    "                              padding=stride // 2, stride=stride,\r\n",
    "                              padding_mode=padding_mode, groups=in_feature)\r\n",
    "        self.fc = nn.Linear(in_feature, out_feature)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "\r\n",
    "        x = self.conv(x)\r\n",
    "\r\n",
    "        return x\r\n",
    "\r\n",
    "\r\n",
    "class conv_embedding(nn.Layer):\r\n",
    "    def __init__(self, in_channels, out_channels, patch_size,\r\n",
    "                 stride, padding):\r\n",
    "        super(conv_embedding, self).__init__()\r\n",
    "        self.conv = nn.Conv2D(in_channels, out_channels, kernel_size=patch_size,\r\n",
    "                              stride=stride, padding=padding, bias_attr=True)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PoolingTransformer(nn.Layer):\r\n",
    "    def __init__(self, image_size, patch_size, stride, base_dims, depth, heads,\r\n",
    "                 mlp_ratio, in_chans=3, attn_drop_rate=.0, drop_rate=.0,\r\n",
    "                 drop_path_rate=.0, class_dim=10):\r\n",
    "        super(PoolingTransformer, self).__init__()\r\n",
    "        self.pool1=paddle.nn.AvgPool2D([1,7],1)\r\n",
    "        self.pool2=paddle.nn.MaxPool2D([7,1],1)\r\n",
    "        total_block = sum(depth)\r\n",
    "        padding = 0\r\n",
    "        block_idx = 0\r\n",
    "\r\n",
    "        width = math.floor(\r\n",
    "            (image_size + 2 * padding - patch_size) / stride + 1)\r\n",
    "\r\n",
    "        self.base_dims = base_dims\r\n",
    "        self.heads = heads\r\n",
    "        self.class_dim = class_dim\r\n",
    "\r\n",
    "        self.patch_size = patch_size\r\n",
    "\r\n",
    "        self.pos_embed = self.create_parameter(\r\n",
    "            shape=(1, base_dims[0] * heads[0], width, width),\r\n",
    "            default_initializer=Assign(\r\n",
    "                paddle.randn((1, base_dims[0] * heads[0], width, width))\r\n",
    "            ))\r\n",
    "        self.add_parameter(\"pos_embed\", self.pos_embed)\r\n",
    "\r\n",
    "        self.patch_embed = conv_embedding(in_chans, base_dims[0] * heads[0],\r\n",
    "                                          patch_size, stride, padding)\r\n",
    "\r\n",
    "        self.cls_token = self.create_parameter(\r\n",
    "            shape=(1, 1, base_dims[0] * heads[0]),\r\n",
    "            default_initializer=Assign(\r\n",
    "                paddle.randn((1, 1, base_dims[0] * heads[0]))\r\n",
    "            ))\r\n",
    "        self.add_parameter(\"cls_token\", self.cls_token)\r\n",
    "\r\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\r\n",
    "\r\n",
    "        self.transformers = nn.LayerList([])\r\n",
    "        self.pools = nn.LayerList([])\r\n",
    "\r\n",
    "        for stage in range(len(depth)):\r\n",
    "            drop_path_prob = [drop_path_rate * i / total_block\r\n",
    "                              for i in range(block_idx, block_idx + depth[stage])]\r\n",
    "            block_idx += depth[stage]\r\n",
    "\r\n",
    "            self.transformers.append(\r\n",
    "                Transformer(base_dims[stage], depth[stage], heads[stage],\r\n",
    "                            mlp_ratio, drop_rate, attn_drop_rate, drop_path_prob)\r\n",
    "            )\r\n",
    "            if stage < len(heads) - 1:\r\n",
    "                self.pools.append(\r\n",
    "                    conv_head_pooling(base_dims[stage] * heads[stage],\r\n",
    "                                      base_dims[stage + 1] * heads[stage + 1],\r\n",
    "                                      stride=2\r\n",
    "                                      )\r\n",
    "                )\r\n",
    "\r\n",
    "        self.norm = nn.LayerNorm(base_dims[-1] * heads[-1], epsilon=1e-6)\r\n",
    "        self.embed_dim = base_dims[-1] * heads[-1]\r\n",
    "\r\n",
    "        # Classifier head\r\n",
    "        if class_dim > 0:\r\n",
    "            self.head = nn.Linear(base_dims[-1] * heads[-1], class_dim)\r\n",
    "\r\n",
    "        trunc_normal_(self.pos_embed)\r\n",
    "        self.apply(self._init_weights)\r\n",
    "\r\n",
    "    def _init_weights(self, m):\r\n",
    "        if isinstance(m, nn.LayerNorm):\r\n",
    "            zeros_(m.bias)\r\n",
    "            ones_(m.weight)\r\n",
    "\r\n",
    "    def forward_features(self, x):\r\n",
    "        x = self.patch_embed(x)\r\n",
    "\r\n",
    "        pos_embed = self.pos_embed\r\n",
    "        x = self.pos_drop(x + pos_embed)\r\n",
    "\r\n",
    "        for stage in range(len(self.pools)):\r\n",
    "            x = self.transformers[stage](x)\r\n",
    "            x = self.pools[stage](x)\r\n",
    "        x = self.transformers[len(\r\n",
    "            self.transformers)-1](x)\r\n",
    "        x=self.pool1(x)\r\n",
    "        x=self.pool2(x)\r\n",
    "        x=x.flatten(1)\r\n",
    "        x = self.norm(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.forward_features(x)\r\n",
    "\r\n",
    "        if self.class_dim > 0:\r\n",
    "            x = self.head(x)\r\n",
    "\r\n",
    "        return x\r\n",
    "\r\n",
    "def pit_s(**kwargs):\r\n",
    "    model = PoolingTransformer(\r\n",
    "        image_size=224,\r\n",
    "        patch_size=16,\r\n",
    "        stride=8,\r\n",
    "        base_dims=[48, 48, 48],\r\n",
    "        depth=[2, 6, 4],\r\n",
    "        heads=[3, 6, 12],\r\n",
    "        mlp_ratio=4,\r\n",
    "        **kwargs\r\n",
    "    )\r\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10]\n"
     ]
    }
   ],
   "source": [
    "model = pit_s()\r\n",
    "out = model(paddle.randn((1, 3, 224, 224)))\r\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cache file /home/aistudio/.cache/paddle/dataset/cifar/cifar-10-python.tar.gz not found, downloading https://dataset.bj.bcebos.com/cifar/cifar-10-python.tar.gz \n",
      "Begin to download\n",
      "\n",
      "Download finished\n"
     ]
    }
   ],
   "source": [
    "import paddle.vision.transforms as T\r\n",
    "from paddle.vision.datasets import Cifar10\r\n",
    "\r\n",
    "paddle.set_device('gpu')\r\n",
    "\r\n",
    "#数据准备\r\n",
    "transform = T.Compose([\r\n",
    "    T.Resize(size=(224,224)),\r\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],data_format='HWC'),\r\n",
    "    T.ToTensor()\r\n",
    "])\r\n",
    "\r\n",
    "train_dataset = Cifar10(mode='train', transform=transform)\r\n",
    "val_dataset = Cifar10(mode='test',  transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SaveBestModel(paddle.callbacks.Callback):\r\n",
    "    def __init__(self, target=0.5, path='./best_model', verbose=0):\r\n",
    "        self.target = target\r\n",
    "        self.epoch = None\r\n",
    "        self.path = path\r\n",
    "\r\n",
    "    def on_epoch_end(self, epoch, logs=None):\r\n",
    "        self.epoch = epoch\r\n",
    "\r\n",
    "    def on_eval_end(self, logs=None):\r\n",
    "        if logs.get('acc') > self.target:\r\n",
    "            self.target = logs.get('acc')\r\n",
    "            self.model.save(self.path)\r\n",
    "            print('best acc is {} at epoch {}'.format(self.target, self.epoch+1))\r\n",
    "callback_savebestmodel = SaveBestModel(target=0.5, path='./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = pit_s()\r\n",
    "model = paddle.Model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3125/3125 [==============================] - loss: 1.0133 - acc: 0.4202 - 86ms/step        \n",
      "save checkpoint at /home/aistudio/checkpoint/0\n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.9148 - acc: 0.5078 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.5078 at epoch 1\n",
      "Epoch 2/50\n",
      "step 3125/3125 [==============================] - loss: 1.3275 - acc: 0.5763 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.2862 - acc: 0.6056 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.6056 at epoch 2\n",
      "Epoch 3/50\n",
      "step 3125/3125 [==============================] - loss: 0.5051 - acc: 0.6410 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.9114 - acc: 0.6341 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.6341 at epoch 3\n",
      "Epoch 4/50\n",
      "step 3125/3125 [==============================] - loss: 0.8642 - acc: 0.6900 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.9024 - acc: 0.6264 - 34ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 5/50\n",
      "step 3125/3125 [==============================] - loss: 0.6675 - acc: 0.7316 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8510 - acc: 0.6651 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.6651 at epoch 5\n",
      "Epoch 6/50\n",
      "step 3125/3125 [==============================] - loss: 0.6180 - acc: 0.7666 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.6556 - acc: 0.6814 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.6814 at epoch 6\n",
      "Epoch 7/50\n",
      "step 3125/3125 [==============================] - loss: 0.6068 - acc: 0.7962 - 85ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8585 - acc: 0.6858 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.6858 at epoch 7\n",
      "Epoch 8/50\n",
      "step 3125/3125 [==============================] - loss: 0.9491 - acc: 0.8283 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8701 - acc: 0.6641 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 9/50\n",
      "step 3125/3125 [==============================] - loss: 0.3699 - acc: 0.8561 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0513 - acc: 0.6876 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.6876 at epoch 9\n",
      "Epoch 10/50\n",
      "step 3125/3125 [==============================] - loss: 0.4465 - acc: 0.8796 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8325 - acc: 0.6836 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 11/50\n",
      "step 3125/3125 [==============================] - loss: 0.1208 - acc: 0.8981 - 87ms/step         \n",
      "save checkpoint at /home/aistudio/checkpoint/10\n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.6829 - acc: 0.7049 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.7049 at epoch 11\n",
      "Epoch 12/50\n",
      "step 3125/3125 [==============================] - loss: 0.2828 - acc: 0.9201 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.7739 - acc: 0.6984 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 13/50\n",
      "step 3125/3125 [==============================] - loss: 0.0898 - acc: 0.9283 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8787 - acc: 0.6903 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 14/50\n",
      "step 3125/3125 [==============================] - loss: 0.0664 - acc: 0.9406 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.6840 - acc: 0.7060 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.706 at epoch 14\n",
      "Epoch 15/50\n",
      "step 3125/3125 [==============================] - loss: 0.0128 - acc: 0.9451 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.7717 - acc: 0.7048 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 16/50\n",
      "step 3125/3125 [==============================] - loss: 0.0214 - acc: 0.9491 - 86ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.7533 - acc: 0.7107 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.7107 at epoch 16\n",
      "Epoch 17/50\n",
      "step 3125/3125 [==============================] - loss: 0.0477 - acc: 0.9535 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.9854 - acc: 0.6970 - 34ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 18/50\n",
      "step 3125/3125 [==============================] - loss: 0.1464 - acc: 0.9593 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.7160 - acc: 0.6971 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 19/50\n",
      "step 3125/3125 [==============================] - loss: 0.1788 - acc: 0.9643 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.7634 - acc: 0.7099 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 20/50\n",
      "step 3125/3125 [==============================] - loss: 0.0227 - acc: 0.9659 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.7381 - acc: 0.6941 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 21/50\n",
      "step 3125/3125 [==============================] - loss: 0.0684 - acc: 0.9677 - 86ms/step        \n",
      "save checkpoint at /home/aistudio/checkpoint/20\n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8415 - acc: 0.6982 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 22/50\n",
      "step 3125/3125 [==============================] - loss: 0.1328 - acc: 0.9675 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0294 - acc: 0.7113 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.7113 at epoch 22\n",
      "Epoch 23/50\n",
      "step 3125/3125 [==============================] - loss: 0.0299 - acc: 0.9707 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.4692 - acc: 0.7099 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 24/50\n",
      "step 3125/3125 [==============================] - loss: 0.2022 - acc: 0.9709 - 86ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8608 - acc: 0.7065 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 25/50\n",
      "step 3125/3125 [==============================] - loss: 0.1582 - acc: 0.9722 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.9843 - acc: 0.7145 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.7145 at epoch 25\n",
      "Epoch 26/50\n",
      "step 3125/3125 [==============================] - loss: 0.0431 - acc: 0.9734 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.9874 - acc: 0.7154 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.7154 at epoch 26\n",
      "Epoch 27/50\n",
      "step 3125/3125 [==============================] - loss: 0.0639 - acc: 0.9754 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.5920 - acc: 0.7157 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.7157 at epoch 27\n",
      "Epoch 28/50\n",
      "step 3125/3125 [==============================] - loss: 0.0444 - acc: 0.9759 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8985 - acc: 0.7163 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.7163 at epoch 28\n",
      "Epoch 29/50\n",
      "step 3125/3125 [==============================] - loss: 0.0290 - acc: 0.9768 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8953 - acc: 0.7077 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 30/50\n",
      "step 3125/3125 [==============================] - loss: 0.0138 - acc: 0.9793 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.7641 - acc: 0.7222 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.7222 at epoch 30\n",
      "Epoch 31/50\n",
      "step 3125/3125 [==============================] - loss: 0.0821 - acc: 0.9765 - 86ms/step        \n",
      "save checkpoint at /home/aistudio/checkpoint/30\n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0239 - acc: 0.7168 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 32/50\n",
      "step 3125/3125 [==============================] - loss: 0.2366 - acc: 0.9794 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0370 - acc: 0.6931 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 33/50\n",
      "step 3125/3125 [==============================] - loss: 0.0388 - acc: 0.9777 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0317 - acc: 0.7113 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 34/50\n",
      "step 3125/3125 [==============================] - loss: 0.0243 - acc: 0.9821 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0022 - acc: 0.7227 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.7227 at epoch 34\n",
      "Epoch 35/50\n",
      "step 3125/3125 [==============================] - loss: 0.0695 - acc: 0.9799 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.9569 - acc: 0.7228 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.7228 at epoch 35\n",
      "Epoch 36/50\n",
      "step 3125/3125 [==============================] - loss: 0.0123 - acc: 0.9796 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.7885 - acc: 0.7163 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 37/50\n",
      "step 3125/3125 [==============================] - loss: 0.0928 - acc: 0.9807 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.6177 - acc: 0.7167 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 38/50\n",
      "step 3125/3125 [==============================] - loss: 0.1633 - acc: 0.9820 - 86ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.9437 - acc: 0.7183 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 39/50\n",
      "step 3125/3125 [==============================] - loss: 0.0385 - acc: 0.9818 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.9333 - acc: 0.7223 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 40/50\n",
      "step 3125/3125 [==============================] - loss: 0.0503 - acc: 0.9816 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.2836 - acc: 0.7264 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.7264 at epoch 40\n",
      "Epoch 41/50\n",
      "step 3125/3125 [==============================] - loss: 0.0199 - acc: 0.9849 - 86ms/step        \n",
      "save checkpoint at /home/aistudio/checkpoint/40\n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.5858 - acc: 0.7083 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 42/50\n",
      "step 3125/3125 [==============================] - loss: 0.0235 - acc: 0.9844 - 86ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.9924 - acc: 0.7224 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 43/50\n",
      "step 3125/3125 [==============================] - loss: 0.0033 - acc: 0.9848 - 86ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.1383 - acc: 0.7285 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "best acc is 0.7285 at epoch 43\n",
      "Epoch 44/50\n",
      "step 3125/3125 [==============================] - loss: 0.0056 - acc: 0.9828 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.3816 - acc: 0.7243 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 45/50\n",
      "step 3125/3125 [==============================] - loss: 0.0264 - acc: 0.9853 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0933 - acc: 0.7259 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 46/50\n",
      "step 3125/3125 [==============================] - loss: 0.0194 - acc: 0.9833 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.8041 - acc: 0.7220 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 47/50\n",
      "step 3125/3125 [==============================] - loss: 0.0047 - acc: 0.9852 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.3249 - acc: 0.7241 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 48/50\n",
      "step 3125/3125 [==============================] - loss: 0.0067 - acc: 0.9860 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0957 - acc: 0.7349 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.7349 at epoch 48\n",
      "Epoch 49/50\n",
      "step 3125/3125 [==============================] - loss: 0.0024 - acc: 0.9848 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.4738 - acc: 0.7183 - 34ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 50/50\n",
      "step 3125/3125 [==============================] - loss: 0.0038 - acc: 0.9846 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.5765 - acc: 0.7359 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "best acc is 0.7359 at epoch 50\n",
      "save checkpoint at /home/aistudio/checkpoint/final\n"
     ]
    }
   ],
   "source": [
    "scheduler = opt.lr.CosineAnnealingDecay(learning_rate=0.000375, T_max=50)\r\n",
    "model.prepare(optimizer=paddle.optimizer.AdamW(weight_decay=0.05,learning_rate=scheduler, parameters=model.parameters()),\r\n",
    "              loss=paddle.nn.CrossEntropyLoss(),\r\n",
    "              metrics=paddle.metric.Accuracy())\r\n",
    "\r\n",
    "visualdl=paddle.callbacks.VisualDL(log_dir='visual_log') # 开启训练可视化\r\n",
    "callbacks=[visualdl,callback_savebestmodel]\r\n",
    "model.fit(\r\n",
    "    train_data=train_dataset, \r\n",
    "    eval_data=val_dataset, \r\n",
    "    batch_size=16, \r\n",
    "    epochs=50,\r\n",
    "    verbose=1,\r\n",
    "    save_dir='checkpoint',\r\n",
    "    save_freq=10,\r\n",
    "    callbacks=callbacks\r\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
